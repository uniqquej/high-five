<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Document</title>
  <script src="face-api.js"></script>
  <script src="https://code.jquery.com/jquery-3.7.0.min.js" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" crossorigin="anonymous"></script>
  <style>
    body {
      margin: 0;
      padding: 0;
      width: 100vw;
      height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    canvas {
      position: absolute;
    }
  </style>
</head>
<body>
  <div class="container">
    <video id="video" width="640" height="480" autoplay playsinline muted style="position:absolute; top:0; left:0;"></video>
    <canvas id="canvas" width="640" height="480" style="position:absolute; top:0; left:0;"></canvas>
    <canvas id="output_canvas" style="position:absolute; top:0; right:0;"></canvas>
  </div>
</body>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
<script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const context = canvas.getContext("2d");
    const WIDTH = canvas.width;
    const HEIGHT = canvas.height;
    let boxWidth, boxHeigth, boxX, boxY;
    let eyeModel;

Promise.all([
  faceapi.nets.tinyFaceDetector.loadFromUri("/")
]).then(startVideo);

async function startVideo() {
  eyeModel = await tf.loadLayersModel('https://raw.githubusercontent.com/noodlej/high-five/main/eyeblink/tfjs_model/model.json');
  stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false })
  video.srcObject = stream;
  console.log('start')
}

video.addEventListener("play", () => {
  const displaySize = { width: WIDTH, height: HEIGHT };

  let interval = setInterval(async () => {
    context.drawImage(video, 0, 0);
    const detections = await faceapi
      .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
    const resizedDetections = faceapi.resizeResults(detections, displaySize);

    if (detections.length != 0){
        video.pause();
        clearInterval(interval)
        const boxInfo = detections[0]._box
        boxWidth = boxInfo.width;
        boxHeight = boxInfo.height;
        boxX=boxInfo._x;
        boxY=boxInfo._y;
        context.beginPath();
        context.lineWidth = 2;
        context.strokeStyle = "#00ff00";
        context.strokeRect(boxInfo._x, boxInfo._y, boxInfo.width, boxInfo.height);

        img = new Image();
			  img.onload = async function(){
				var canvas = document.getElementById("output_canvas");
				canvas.width = boxWidth;
				canvas.height = boxHeight;
				var ctx = canvas.getContext("2d");
				ctx.drawImage( img, boxX, boxY, boxWidth, boxHeight, 0, 0, boxWidth, boxHeight );

        let tensor = tf.browser.fromPixels(img);
        tensor = tensor.resizeNearestNeighbor([224, 224]).div(255.0);
        let x_test = tensor.expandDims(0);
        console.log(x_test)
   
        const pred = eyeModel.predict(x_test).arraySync();;
        const { values, indices } = tf.topk(pred, 2);
        const index = indices.arraySync()[0];
        const probability = values.arraySync()[0];
        console.log(index, probability);
			};
			  img.src = canvas.toDataURL();
    }
    console.log(resizedDetections)}, 100);
});
</script>
</html>